{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --upgrade --quiet  langchain langchain-community langchainhub langchain-openai langchain-chroma bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"ollama2\"\n",
    "\n",
    "# import dotenv\n",
    "\n",
    "# dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"ls__edc251b963974e33be6863ff130069e0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import  OllamaEmbeddings\n",
    "\n",
    "# Load, chunk and index the contents of the blog.\n",
    "bs_strainer = bs4.SoupStrainer(class_=(\"post-content\", \"post-title\", \"post-header\"))\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs={\"parse_only\": bs_strainer},\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OllamaEmbeddings(base_url=\"http://localhost:11434\", model=\"llama2\"))\n",
    "\n",
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "retriever = vectorstore.as_retriever()\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "llm = ChatOpenAI(model=\"llama2\", temperature=0,base_url=\"http://localhost:11434/v1\")\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain_from_docs = (\n",
    "    RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain_with_source = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ").assign(answer=rag_chain_from_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in rag_chain_with_source.stream(\"What is Task Decomposition\"):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {}\n",
    "curr_key = None\n",
    "for chunk in rag_chain_with_source.stream(\"What is Task Decomposition\"):\n",
    "    for key in chunk:\n",
    "        if key not in output:\n",
    "            output[key] = chunk[key]\n",
    "        else:\n",
    "            output[key] += chunk[key]\n",
    "        if key != curr_key:\n",
    "            print(f\"\\n\\n{key}: {chunk[key]}\", end=\"\", flush=True)\n",
    "        else:\n",
    "            print(chunk[key], end=\"\", flush=True)\n",
    "        curr_key = key\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.tracers.log_stream import LogStreamCallbackHandler\n",
    "\n",
    "contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
    "which might reference context in the chat history, formulate a standalone question \\\n",
    "which can be understood without the chat history. Do NOT answer the question, \\\n",
    "just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "contextualize_q_chain = (contextualize_q_prompt | llm | StrOutputParser()).with_config(\n",
    "    tags=[\"contextualize_q_chain\"]\n",
    ")\n",
    "\n",
    "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
    "Use the following pieces of retrieved context to answer the question. \\\n",
    "If you don't know the answer, just say that you don't know. \\\n",
    "Use three sentences maximum and keep the answer concise.\\\n",
    "\n",
    "{context}\"\"\"\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def contextualized_question(input: dict):\n",
    "    if input.get(\"chat_history\"):\n",
    "        return contextualize_q_chain\n",
    "    else:\n",
    "        return input[\"question\"]\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    RunnablePassthrough.assign(context=contextualize_q_chain | retriever | format_docs)\n",
    "    | qa_prompt\n",
    "    | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'LogEntry' from 'logging' (D:\\soft\\python3.11\\Lib\\logging\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List, Any, Dict, Optional, TypedDict\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LogEntry\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mRunState\u001b[39;00m(TypedDict):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mid\u001b[39m: \u001b[38;5;28mstr\u001b[39m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'LogEntry' from 'logging' (D:\\soft\\python3.11\\Lib\\logging\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from typing import List, Any, Dict, Optional, TypedDict\n",
    "class RunState(TypedDict):\n",
    "\n",
    "    id: str\n",
    "    \"\"\"ID of the run.\"\"\"\n",
    "    streamed_output: List[Any]\n",
    "    \"\"\"List of output chunks streamed by Runnable.stream()\"\"\"\n",
    "    final_output: Optional[Any]\n",
    "    \"\"\"Final output of the run, usually the result of aggregating (`+`) streamed_output.\n",
    "    Only available after the run has finished successfully.\"\"\"\n",
    "\n",
    "    logs: Dict[str, LogEntry]\n",
    "    \"\"\"Map of run names to sub-runs. If filters were supplied, this list will\n",
    "    contain only the runs that matched the filters.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed for running async functions in Jupyter notebook:\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunLogPatch({'op': 'replace',\n",
      "  'path': '',\n",
      "  'value': {'final_output': None,\n",
      "            'id': 'bc72b783-8998-4b66-81aa-575307a6d2d5',\n",
      "            'logs': {},\n",
      "            'name': 'RunnableSequence',\n",
      "            'streamed_output': [],\n",
      "            'type': 'chain'}})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/RunnableSequence',\n",
      "  'value': {'end_time': None,\n",
      "            'final_output': None,\n",
      "            'id': 'ba91c21a-450b-417f-ba1d-8ea7d7784d9c',\n",
      "            'metadata': {},\n",
      "            'name': 'RunnableSequence',\n",
      "            'start_time': '2024-04-18T03:52:41.321+00:00',\n",
      "            'streamed_output': [],\n",
      "            'streamed_output_str': [],\n",
      "            'tags': ['seq:step:1', 'contextualize_q_chain'],\n",
      "            'type': 'chain'}})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatPromptTemplate',\n",
      "  'value': {'end_time': None,\n",
      "            'final_output': None,\n",
      "            'id': '1415f242-2232-4297-96e9-a62736aeeaec',\n",
      "            'metadata': {},\n",
      "            'name': 'ChatPromptTemplate',\n",
      "            'start_time': '2024-04-18T03:52:41.328+00:00',\n",
      "            'streamed_output': [],\n",
      "            'streamed_output_str': [],\n",
      "            'tags': ['seq:step:1', 'contextualize_q_chain'],\n",
      "            'type': 'prompt'}})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatPromptTemplate/final_output',\n",
      "  'value': ChatPromptValue(messages=[SystemMessage(content='Given a chat history and the latest user question which might reference context in the chat history, formulate a standalone question which can be understood without the chat history. Do NOT answer the question, just reformulate it if needed and otherwise return it as is.'), HumanMessage(content='What is Task Decomposition?'), AIMessage(content='Task decomposition is a process in which a complex task is broken down into smaller, more manageable sub-tasks or steps. This allows for a more structured and efficient approach to completing the task, as well as providing a clear understanding of the individual components involved. In the context of this benchmark, task decomposition refers to the ability of an agent to break down a complex user request into smaller, more manageable tasks that can be addressed through the use of APIs.', response_metadata={'token_usage': {'completion_tokens': 95, 'prompt_tokens': 818, 'total_tokens': 913}, 'model_name': 'llama2', 'system_fingerprint': 'fp_ollama', 'finish_reason': 'stop', 'logprobs': None}, id='run-384e61f8-a23c-4c1f-953a-7c160838728b-0'), HumanMessage(content='What are common ways of doing it?')])},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatPromptTemplate/end_time',\n",
      "  'value': '2024-04-18T03:52:41.337+00:00'})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI',\n",
      "  'value': {'end_time': None,\n",
      "            'final_output': None,\n",
      "            'id': 'b8408811-56c7-4337-8b83-dde963fffb6e',\n",
      "            'metadata': {},\n",
      "            'name': 'ChatOpenAI',\n",
      "            'start_time': '2024-04-18T03:52:41.347+00:00',\n",
      "            'streamed_output': [],\n",
      "            'streamed_output_str': [],\n",
      "            'tags': ['seq:step:2', 'contextualize_q_chain'],\n",
      "            'type': 'llm'}})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'There'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='There', id='run-b8408811-56c7-4337-8b83-dde963fffb6e')})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser',\n",
      "  'value': {'end_time': None,\n",
      "            'final_output': None,\n",
      "            'id': 'f613d729-3c2f-4c78-8bb9-7a3fe10e72c0',\n",
      "            'metadata': {},\n",
      "            'name': 'StrOutputParser',\n",
      "            'start_time': '2024-04-18T03:53:04.027+00:00',\n",
      "            'streamed_output': [],\n",
      "            'streamed_output_str': [],\n",
      "            'tags': ['seq:step:3', 'contextualize_q_chain'],\n",
      "            'type': 'parser'}})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'There'})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/RunnableSequence/streamed_output/-',\n",
      "  'value': 'There'})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' are'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' are', id='run-b8408811-56c7-4337-8b83-dde963fffb6e')})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' are'})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/RunnableSequence/streamed_output/-',\n",
      "  'value': ' are'})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' several'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' several', id='run-b8408811-56c7-4337-8b83-dde963fffb6e')})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' several'})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/RunnableSequence/streamed_output/-',\n",
      "  'value': ' several'})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' common'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' common', id='run-b8408811-56c7-4337-8b83-dde963fffb6e')})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' common'})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/RunnableSequence/streamed_output/-',\n",
      "  'value': ' common'})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' ways'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' ways', id='run-b8408811-56c7-4337-8b83-dde963fffb6e')})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' ways'})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/RunnableSequence/streamed_output/-',\n",
      "  'value': ' ways'})\n",
      "\n",
      "------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "question = \"What is Task Decomposition?\"\n",
    "ai_msg = rag_chain.invoke({\"question\": question, \"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(content=question), ai_msg])\n",
    "\n",
    "second_question = \"What are common ways of doing it?\"\n",
    "ct = 0\n",
    "async for jsonpatch_op in rag_chain.astream_log(\n",
    "    {\"question\": second_question, \"chat_history\": chat_history},\n",
    "    include_tags=[\"contextualize_q_chain\"],\n",
    "):\n",
    "    print(jsonpatch_op)\n",
    "    print(\"\\n\" + \"-\" * 30 + \"\\n\")\n",
    "    ct += 1\n",
    "    if ct > 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunLogPatch({'op': 'replace',\n",
      "  'path': '',\n",
      "  'value': {'final_output': None,\n",
      "            'id': '56c5b3e2-aa46-4ad8-85d9-f448adb81a96',\n",
      "            'logs': {},\n",
      "            'name': 'RunnableSequence',\n",
      "            'streamed_output': [],\n",
      "            'type': 'chain'}})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/Retriever',\n",
      "  'value': {'end_time': None,\n",
      "            'final_output': None,\n",
      "            'id': '60f0628b-f9f2-4bf8-a01b-fc2f44422ea1',\n",
      "            'metadata': {},\n",
      "            'name': 'Retriever',\n",
      "            'start_time': '2024-04-18T05:37:00.785+00:00',\n",
      "            'streamed_output': [],\n",
      "            'streamed_output_str': [],\n",
      "            'tags': ['seq:step:2', 'Chroma', 'OllamaEmbeddings'],\n",
      "            'type': 'retriever'}})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/Retriever/final_output',\n",
      "  'value': {'documents': [Document(page_content='Resources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
      "                          Document(page_content='Resources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
      "                          Document(page_content='This benchmark evaluates the agent’s tool use capabilities at three levels:\\n\\nLevel-1 evaluates the ability to call the API. Given an API’s description, the model needs to determine whether to call a given API, call it correctly, and respond properly to API returns.\\nLevel-2 examines the ability to retrieve the API. The model needs to search for possible APIs that may solve the user’s requirement and learn how to use them by reading documentation.\\nLevel-3 assesses the ability to plan API beyond retrieve and call. Given unclear user requests (e.g. schedule group meetings, book flight/hotel/restaurant for a trip), the model may have to conduct multiple API calls to solve it.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
      "                          Document(page_content='This benchmark evaluates the agent’s tool use capabilities at three levels:\\n\\nLevel-1 evaluates the ability to call the API. Given an API’s description, the model needs to determine whether to call a given API, call it correctly, and respond properly to API returns.\\nLevel-2 examines the ability to retrieve the API. The model needs to search for possible APIs that may solve the user’s requirement and learn how to use them by reading documentation.\\nLevel-3 assesses the ability to plan API beyond retrieve and call. Given unclear user requests (e.g. schedule group meetings, book flight/hotel/restaurant for a trip), the model may have to conduct multiple API calls to solve it.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'})]}},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/Retriever/end_time',\n",
      "  'value': '2024-04-18T05:37:48.700+00:00'})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': AIMessageChunk(content='There', id='run-cacd509a-abe1-4603-bb77-8a6652f86e7e')})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': AIMessageChunk(content='There are', id='run-cacd509a-abe1-4603-bb77-8a6652f86e7e')})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': AIMessageChunk(content='There are several', id='run-cacd509a-abe1-4603-bb77-8a6652f86e7e')})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': AIMessageChunk(content='There are several common', id='run-cacd509a-abe1-4603-bb77-8a6652f86e7e')})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': AIMessageChunk(content='There are several common ways', id='run-cacd509a-abe1-4603-bb77-8a6652f86e7e')})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': AIMessageChunk(content='There are several common ways to', id='run-cacd509a-abe1-4603-bb77-8a6652f86e7e')})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': AIMessageChunk(content='There are several common ways to de', id='run-cacd509a-abe1-4603-bb77-8a6652f86e7e')})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': AIMessageChunk(content='There are several common ways to decompose', id='run-cacd509a-abe1-4603-bb77-8a6652f86e7e')})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': AIMessageChunk(content='There are several common ways to decompose a', id='run-cacd509a-abe1-4603-bb77-8a6652f86e7e')})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': AIMessageChunk(content='There are several common ways to decompose a task', id='run-cacd509a-abe1-4603-bb77-8a6652f86e7e')})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': AIMessageChunk(content='There are several common ways to decompose a task:', id='run-cacd509a-abe1-4603-bb77-8a6652f86e7e')})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': AIMessageChunk(content='There are several common ways to decompose a task:\\n', id='run-cacd509a-abe1-4603-bb77-8a6652f86e7e')})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': AIMessageChunk(content='There are several common ways to decompose a task:\\n\\n', id='run-cacd509a-abe1-4603-bb77-8a6652f86e7e')})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': AIMessageChunk(content='There are several common ways to decompose a task:\\n\\n1', id='run-cacd509a-abe1-4603-bb77-8a6652f86e7e')})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': AIMessageChunk(content='There are several common ways to decompose a task:\\n\\n1.', id='run-cacd509a-abe1-4603-bb77-8a6652f86e7e')})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': AIMessageChunk(content='There are several common ways to decompose a task:\\n\\n1. Break', id='run-cacd509a-abe1-4603-bb77-8a6652f86e7e')})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': AIMessageChunk(content='There are several common ways to decompose a task:\\n\\n1. Break down', id='run-cacd509a-abe1-4603-bb77-8a6652f86e7e')})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': AIMessageChunk(content='There are several common ways to decompose a task:\\n\\n1. Break down the', id='run-cacd509a-abe1-4603-bb77-8a6652f86e7e')})\n",
      "\n",
      "------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ct = 0\n",
    "async for jsonpatch_op in rag_chain.astream_log(\n",
    "    {\"question\": second_question, \"chat_history\": chat_history},\n",
    "    include_names=[\"Retriever\"],\n",
    "    with_streamed_output_list=False,\n",
    "):\n",
    "    print(jsonpatch_op)\n",
    "    print(\"\\n\" + \"-\" * 30 + \"\\n\")\n",
    "    ct += 1\n",
    "    if ct > 20:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
