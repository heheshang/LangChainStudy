{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 01 langchain结合本地知识库问答案例\n",
    "https://python.langchain.com/docs/use_cases/question_answering/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-04T20:08:04.034389Z",
     "start_time": "2023-08-04T20:08:01.862026Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting langchain\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/4b/28/da40a6b12e7842a0c8b443f8cc5c6f59e49d7a9071cfad064b9639c6b044/langchain-0.1.20-py3-none-any.whl (1.0 MB)\n",
      "     ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "     ---- ----------------------------------- 0.1/1.0 MB 2.4 MB/s eta 0:00:01\n",
      "     --------------- ------------------------ 0.4/1.0 MB 4.1 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 0.7/1.0 MB 5.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  1.0/1.0 MB 5.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.0/1.0 MB 5.4 MB/s eta 0:00:00\n",
      "Collecting PyYAML>=5.3 (from langchain)\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/b3/34/65bb4b2d7908044963ebf614fe0fdb080773fc7030d7e39c8d3eddcd4257/PyYAML-6.0.1-cp311-cp311-win_amd64.whl (144 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/74/9a/eec023807ae78e83342567303916b34a348d9d40703e7cef5dfb1e3635b6/SQLAlchemy-2.0.30-cp311-cp311-win_amd64.whl (2.1 MB)\n",
      "     ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "     ------ --------------------------------- 0.3/2.1 MB 6.5 MB/s eta 0:00:01\n",
      "     ------------ --------------------------- 0.7/2.1 MB 8.4 MB/s eta 0:00:01\n",
      "     ------------------- -------------------- 1.0/2.1 MB 7.1 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 1.4/2.1 MB 7.4 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 1.6/2.1 MB 7.0 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 2.0/2.1 MB 7.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.1/2.1 MB 6.6 MB/s eta 0:00:00\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain)\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/a4/69/0d415c6d8450842652ce01b29f43416a0f30122b75899de01485623c7850/aiohttp-3.9.5-cp311-cp311-win_amd64.whl (370 kB)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/92/70/382283d80cb998ebc0089428b109bbe606ec9dce891a3cb1468c03ed0ad6/dataclasses_json-0.6.6-py3-none-any.whl (28 kB)\n",
      "Collecting langchain-community<0.1,>=0.0.38 (from langchain)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/1b/d3/1f4d1941ae5a627299c8ea052847b99ad6674b97b699d8a08fc4faf25d3e/langchain_community-0.0.38-py3-none-any.whl (2.0 MB)\n",
      "     ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "     -- ------------------------------------- 0.1/2.0 MB 7.5 MB/s eta 0:00:01\n",
      "     ----------- ---------------------------- 0.6/2.0 MB 5.9 MB/s eta 0:00:01\n",
      "     ----------------- ---------------------- 0.9/2.0 MB 7.0 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 1.2/2.0 MB 6.6 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 1.5/2.0 MB 6.6 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 1.9/2.0 MB 7.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.0/2.0 MB 6.5 MB/s eta 0:00:00\n",
      "Collecting langchain-core<0.2.0,>=0.1.52 (from langchain)\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/43/8b/48b7e6de9041d2b33d5108e154b82d1bd6c47cc68f0e44cb4fcdaccf5ec7/langchain_core-0.1.52-py3-none-any.whl (302 kB)\n",
      "Collecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/9d/a1/aec824080111e9b4a4802b51b988032faa193828c865e11233d1b18e88fa/langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/fc/10/b09b5cf70cb315b480fe9a814d1045149a9b923201e32daa684068a93ca7/langsmith-0.1.57-py3-none-any.whl (121 kB)\n",
      "     ---------------------------------------- 0.0/121.0 kB ? eta -:--:--\n",
      "     -------------------------------------- 121.0/121.0 kB 7.4 MB/s eta 0:00:00\n",
      "Collecting numpy<2,>=1 (from langchain)\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/3f/6b/5610004206cf7f8e7ad91c5a85a8c71b2f2f8051a0c0c4d5916b76d6cbb2/numpy-1.26.4-cp311-cp311-win_amd64.whl (15.8 MB)\n",
      "Collecting pydantic<3,>=1 (from langchain)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/ed/76/9a17032880ed27f2dbd490c77a3431cbc80f47ba81534131de3c2846e736/pydantic-2.7.1-py3-none-any.whl (409 kB)\n",
      "     ---------------------------------------- 0.0/409.3 kB ? eta -:--:--\n",
      "     ----------------------- -------------- 256.0/409.3 kB 7.7 MB/s eta 0:00:01\n",
      "     -------------------------------------- 409.3/409.3 kB 6.3 MB/s eta 0:00:00\n",
      "Collecting requests<3,>=2 (from langchain)\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/70/8e/0e2d847013cb52cd35b38c009bb167a1a26b2ce6cd6965bf26b47bc0bf44/requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Collecting tenacity<9.0.0,>=8.1.0 (from langchain)\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/61/a1/6bb0cbebefb23641f068bb58a2bc56da9beb2b1c550242e3c540b37698f3/tenacity-8.3.0-py3-none-any.whl (25 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/76/ac/a7305707cb852b7e16ff80eaf5692309bde30e2b1100a1fcacdc8f731d97/aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/e0/44/827b2a91a5816512fcaf3cc4ebc465ccd5d598c45cefa6703fcf4a79018f/attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/b3/21/c5aaffac47fd305d69df46cfbf118768cdf049a92ee6b0b5cb029d449dcf/frozenlist-1.4.1-cp311-cp311-win_amd64.whl (50 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/88/aa/ea217cb18325aa05cb3e3111c19715f1e97c50a4a900cbc20e54648de5f5/multidict-6.0.5-cp311-cp311-win_amd64.whl (28 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/27/41/945ae9a80590e4fb0be166863c6e63d75e4b35789fa3a61ff1dbdcdc220f/yarl-1.9.4-cp311-cp311-win_amd64.whl (76 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/be/24/cbb242420021a79c87768dcd22ce028f48ef40913239ad6106c8a557f52c/marshmallow-3.21.2-py3-none-any.whl (49 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/65/f3/107a22063bf27bdccf2024833d3445f4eea42b2e598abfbd46f6a63b6cb0/typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.2.0,>=0.1.52->langchain)\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/73/07/02e16ed01e04a374e644b575638ec7987ae846d25ad97bcc9945a3ee4b0e/jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Collecting packaging<24.0,>=23.2 (from langchain-core<0.2.0,>=0.1.52->langchain)\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/ec/1a/610693ac4ee14fcdf2d9bf3c493370e4f2ef7ae2e19217d7a237ff42367d/packaging-23.2-py3-none-any.whl (53 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/f9/b7/3815984df03b677644c90cd4893d6293c80ef1c9f3a8493807bc1eb47da7/orjson-3.10.3-cp311-none-win_amd64.whl (138 kB)\n",
      "     ---------------------------------------- 0.0/138.8 kB ? eta -:--:--\n",
      "     -------------------- ------------------- 71.7/138.8 kB ? eta -:--:--\n",
      "     -------------------------------------- 138.8/138.8 kB 2.7 MB/s eta 0:00:00\n",
      "Collecting annotated-types>=0.4.0 (from pydantic<3,>=1->langchain)\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/28/78/d31230046e58c207284c6b2c4e8d96e6d3cb4e52354721b944d3e1ee4aa5/annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Collecting pydantic-core==2.18.2 (from pydantic<3,>=1->langchain)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/9d/b0/e8bebe8fd08ea6ec027b7304c84f4652f2933514caf9f6a418d259d2a950/pydantic_core-2.18.2-cp311-none-win_amd64.whl (1.9 MB)\n",
      "     ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "     --- ------------------------------------ 0.2/1.9 MB 4.6 MB/s eta 0:00:01\n",
      "     ----- ---------------------------------- 0.3/1.9 MB 3.5 MB/s eta 0:00:01\n",
      "     ----------- ---------------------------- 0.5/1.9 MB 4.2 MB/s eta 0:00:01\n",
      "     ------------- -------------------------- 0.7/1.9 MB 3.8 MB/s eta 0:00:01\n",
      "     ----------------- ---------------------- 0.8/1.9 MB 3.8 MB/s eta 0:00:01\n",
      "     ------------------- -------------------- 0.9/1.9 MB 3.3 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 1.1/1.9 MB 3.6 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 1.4/1.9 MB 3.6 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 1.5/1.9 MB 3.5 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 1.6/1.9 MB 3.4 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 1.8/1.9 MB 3.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  1.9/1.9 MB 3.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------  1.9/1.9 MB 3.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------  1.9/1.9 MB 3.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------  1.9/1.9 MB 3.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------  1.9/1.9 MB 3.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.9/1.9 MB 2.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from pydantic<3,>=1->langchain) (4.11.0)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3,>=2->langchain)\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/57/ec/80c8d48ac8b1741d5b963797b7c0c869335619e13d4744ca2f67fc11c6fc/charset_normalizer-3.3.2-cp311-cp311-win_amd64.whl (99 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3,>=2->langchain)\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/e5/3e/741d8c82801c347547f8a2a06aa57dbb1992be9e948df2ea0eda2c8b79e8/idna-3.7-py3-none-any.whl (66 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2->langchain)\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/a2/73/a68704750a7679d0b6d3ad7aa8d4da8e14e151ae82e6fee774e6e0d05ec8/urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3,>=2->langchain)\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/ba/06/a07f096c664aeb9f01624f858c3add0a4e913d6c96257acb4fce61e7de14/certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->langchain)\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/47/79/26d54d7d700ef65b689fc2665a40846d13e834da0486674a8d4f0f371a47/greenlet-3.0.3-cp311-cp311-win_amd64.whl (292 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.52->langchain)\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/12/f6/0232cc0c617e195f06f810534d00b74d2f348fe71b2118009ad8ad31f878/jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/2a/e2/5d3f6ada4297caebe1a2add3b126fe800c96f56dbe5d1988a2cbe0b267aa/mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: urllib3, tenacity, PyYAML, pydantic-core, packaging, orjson, numpy, mypy-extensions, multidict, jsonpointer, idna, greenlet, frozenlist, charset-normalizer, certifi, attrs, annotated-types, yarl, typing-inspect, SQLAlchemy, requests, pydantic, marshmallow, jsonpatch, aiosignal, langsmith, dataclasses-json, aiohttp, langchain-core, langchain-text-splitters, langchain-community, langchain\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 24.0\n",
      "    Uninstalling packaging-24.0:\n",
      "      Successfully uninstalled packaging-24.0\n",
      "Successfully installed PyYAML-6.0.1 SQLAlchemy-2.0.30 aiohttp-3.9.5 aiosignal-1.3.1 annotated-types-0.6.0 attrs-23.2.0 certifi-2024.2.2 charset-normalizer-3.3.2 dataclasses-json-0.6.6 frozenlist-1.4.1 greenlet-3.0.3 idna-3.7 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.20 langchain-community-0.0.38 langchain-core-0.1.52 langchain-text-splitters-0.0.1 langsmith-0.1.57 marshmallow-3.21.2 multidict-6.0.5 mypy-extensions-1.0.0 numpy-1.26.4 orjson-3.10.3 packaging-23.2 pydantic-2.7.1 pydantic-core-2.18.2 requests-2.31.0 tenacity-8.3.0 typing-inspect-0.9.0 urllib3-2.2.1 yarl-1.9.4\n"
     ]
    }
   ],
   "source": [
    "! pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-04T20:08:08.843390Z",
     "start_time": "2023-08-04T20:08:04.038392Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting openai\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/1d/f8/e36df35b6c3f5fd73cdc655a717931a1d035b441685bc8f7b4aa6076a780/openai-1.29.0-py3-none-any.whl (320 kB)\n",
      "     ---------------------------------------- 0.0/320.3 kB ? eta -:--:--\n",
      "     --------------------------- ---------- 235.5/320.3 kB 7.3 MB/s eta 0:00:01\n",
      "     ---------------------------------- --- 286.7/320.3 kB 5.9 MB/s eta 0:00:01\n",
      "     ---------------------------------- --- 286.7/320.3 kB 5.9 MB/s eta 0:00:01\n",
      "     ---------------------------------- --- 286.7/320.3 kB 5.9 MB/s eta 0:00:01\n",
      "     ---------------------------------- --- 286.7/320.3 kB 5.9 MB/s eta 0:00:01\n",
      "     ---------------------------------- --- 286.7/320.3 kB 5.9 MB/s eta 0:00:01\n",
      "     -------------------------------------- 320.3/320.3 kB 1.1 MB/s eta 0:00:00\n",
      "Collecting anyio<5,>=3.5.0 (from openai)\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/14/fd/2f20c40b45e4fb4324834aea24bd4afdf1143390242c0b33774da0e2e34f/anyio-4.3.0-py3-none-any.whl (85 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/12/b3/231ffd4ab1fc9d679809f356cebee130ac7daa00d6d6f3206dd4fd137e9e/distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/41/7b/ddacf6dcebb42466abd03f368782142baa82e08fc0c1f8eaa05b4bae87d5/httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from openai) (2.7.1)\n",
      "Collecting sniffio (from openai)\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/e9/44/75a9c9421471a6c4805dbf2356f7c181a29c1879239abab1ea2cc8f38b40/sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Collecting tqdm>4 (from openai)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/18/eb/fdb7eb9e48b7b02554e1664afd3bd3f117f6b6d6c5881438a0b055554f9b/tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "     ---------------------------------------- 0.0/78.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 78.3/78.3 kB 4.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/78/d4/e5d7e4f2174f8a4d63c8897d79eb8fe2503f7ecc03282fee1fa2719c2704/httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/95/04/ff642e65ad6b90db43e668d70ffb6736436c7ce41fcc549f4e9472234127/h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.18.2)\n",
      "Requirement already satisfied: colorama in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Installing collected packages: tqdm, sniffio, h11, distro, httpcore, anyio, httpx, openai\n",
      "Successfully installed anyio-4.3.0 distro-1.9.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.29.0 sniffio-1.3.1 tqdm-4.66.4\n"
     ]
    }
   ],
   "source": [
    "! pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-04T22:05:55.066278Z",
     "start_time": "2023-08-04T22:05:55.038770Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(openai_api_key=\"sk-y09fqxxxxxxxxxz9rA7fU\")#这里需要填入你的openai api key，这个api已经不可用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-04T21:40:09.506487Z",
     "start_time": "2023-08-04T21:38:13.338582Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: openai in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (1.29.0)\n",
      "Requirement already satisfied: chromadb in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (0.4.24)\n",
      "Collecting tiktoken\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/b1/10/c04b4ff592a5f46b28ebf4c2353f735c02ae7f0ce1b165d00748ced6467e/tiktoken-0.7.0-cp311-cp311-win_amd64.whl (799 kB)\n",
      "     ---------------------------------------- 0.0/799.0 kB ? eta -:--:--\n",
      "     ----- -------------------------------- 122.9/799.0 kB 7.0 MB/s eta 0:00:01\n",
      "     ----------- -------------------------- 235.5/799.0 kB 2.4 MB/s eta 0:00:01\n",
      "     ---------------------- --------------- 471.0/799.0 kB 3.3 MB/s eta 0:00:01\n",
      "     ---------------------------------- --- 716.8/799.0 kB 3.8 MB/s eta 0:00:01\n",
      "     -------------------------------------- 799.0/799.0 kB 3.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from openai) (4.3.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from openai) (2.7.1)\n",
      "Requirement already satisfied: sniffio in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from openai) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: build>=1.0.3 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from chromadb) (1.2.1)\n",
      "Requirement already satisfied: requests>=2.28 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from chromadb) (2.31.0)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.3 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from chromadb) (0.7.3)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from chromadb) (0.111.0)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.29.0)\n",
      "Requirement already satisfied: numpy>=1.22.5 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from chromadb) (1.26.4)\n",
      "Requirement already satisfied: posthog>=2.4.0 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from chromadb) (3.5.0)\n",
      "Requirement already satisfied: pulsar-client>=3.1.0 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from chromadb) (3.5.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from chromadb) (1.17.3)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from chromadb) (1.24.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from chromadb) (1.24.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from chromadb) (0.45b0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from chromadb) (1.24.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from chromadb) (0.19.1)\n",
      "Requirement already satisfied: pypika>=0.48.9 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: overrides>=7.3.1 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from chromadb) (6.4.0)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from chromadb) (1.63.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from chromadb) (4.1.3)\n",
      "Requirement already satisfied: typer>=0.9.0 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from chromadb) (0.12.3)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from chromadb) (29.0.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from chromadb) (8.3.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from chromadb) (6.0.1)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from chromadb) (4.1.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from chromadb) (3.10.3)\n",
      "Collecting regex>=2022.1.18 (from tiktoken)\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/47/34/8dd49be3224d010479c80b5e758a5a25322cb2d082eb7ecb0e9e29738dc4/regex-2024.5.10-cp311-cp311-win_amd64.whl (268 kB)\n",
      "Requirement already satisfied: idna>=2.8 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: packaging>=19.1 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from build>=1.0.3->chromadb) (23.2)\n",
      "Requirement already satisfied: pyproject_hooks in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from build>=1.0.3->chromadb) (1.1.0)\n",
      "Requirement already satisfied: colorama in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from build>=1.0.3->chromadb) (0.4.6)\n",
      "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from fastapi>=0.95.2->chromadb) (0.37.2)\n",
      "Requirement already satisfied: fastapi-cli>=0.0.2 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from fastapi>=0.95.2->chromadb) (0.0.3)\n",
      "Requirement already satisfied: jinja2>=2.11.2 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from fastapi>=0.95.2->chromadb) (3.1.4)\n",
      "Requirement already satisfied: python-multipart>=0.0.7 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from fastapi>=0.95.2->chromadb) (0.0.9)\n",
      "Requirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from fastapi>=0.95.2->chromadb) (5.9.0)\n",
      "Requirement already satisfied: email_validator>=2.0.0 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from fastapi>=0.95.2->chromadb) (2.1.1)\n",
      "Requirement already satisfied: certifi in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: six>=1.9.0 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.29.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: coloredlogs in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n",
      "Requirement already satisfied: protobuf in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (4.25.3)\n",
      "Requirement already satisfied: sympy in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.14)\n",
      "Requirement already satisfied: importlib-metadata<=7.0,>=6.0 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (7.0.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.63.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.24.0 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.24.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.24.0 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.24.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.45b0 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.45b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.45b0 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.45b0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.45b0 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.45b0)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.45b0 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.45b0)\n",
      "Requirement already satisfied: setuptools>=16.0 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from opentelemetry-instrumentation==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (65.5.0)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from opentelemetry-instrumentation==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: asgiref~=3.0 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from opentelemetry-instrumentation-asgi==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
      "Requirement already satisfied: monotonic>=1.5 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.18.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from requests>=2.28->chromadb) (3.3.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from tokenizers>=0.13.2->chromadb) (0.23.0)\n",
      "Requirement already satisfied: click>=8.0.0 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from typer>=0.9.0->chromadb) (13.7.1)\n",
      "Requirement already satisfied: httptools>=0.5.0 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.1)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\n",
      "Requirement already satisfied: watchfiles>=0.13 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
      "Requirement already satisfied: websockets>=10.4 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (12.0)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from email_validator>=2.0.0->fastapi>=0.95.2->chromadb) (2.6.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
      "Requirement already satisfied: filelock in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2024.3.1)\n",
      "Requirement already satisfied: zipp>=0.5 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from importlib-metadata<=7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.18.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from jinja2>=2.11.2->fastapi>=0.95.2->chromadb) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (2.18.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: pyreadline3 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb) (3.4.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in d:\\soft\\workspace\\pythonworkspace\\langchainstudy\\.venv\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.0)\n",
      "Installing collected packages: regex, tiktoken\n",
      "Successfully installed regex-2024.5.10 tiktoken-0.7.0\n"
     ]
    }
   ],
   "source": [
    "! pip install openai chromadb tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## sqlite 安装\n",
    "在 Windows 上安装 SQLite\n",
    "\n",
    "请访问 SQLite 下载页面[https://www.sqlite.org/download.html] ，从 Windows 区下载预编译的二进制文件。\n",
    "\n",
    "您需要下载 sqlite-tools-win32-*.zip 和 sqlite-dll-win64-*.zip 压缩文件。\n",
    "\n",
    "创建文件夹 C:\\sqlite，并在此文件夹下解压上面两个压缩文件，将得到 sqlite3.def、sqlite3.dll 和 sqlite3.exe 文件。\n",
    "\n",
    "添加 C:\\sqlite 到 PATH 环境变量，最后在命令提示符下，使用 sqlite3 命令，将显示如下结果。\n",
    "\n",
    "```shell\n",
    "C:\\>sqlite3\n",
    "SQLite version 3.7.15.2 2013-01-09 11:53:05\n",
    "Enter \".help\" for instructions\n",
    "Enter SQL statements terminated with a \";\"\n",
    "sqlite>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 环境变量设置\n",
    "第一步，按下“win+r”键，打开运行框，\n",
    "\n",
    "第二步，输入命令“control system”,\n",
    "\n",
    "第三步，在控制面板主页，打开“高级系统设置”，\n",
    "\n",
    "第四步，在高级系统设置界面，选择“环境变量”选项，\n",
    "\n",
    "第五步，根据需求进行设置。\n",
    "\n",
    "### 如果遇到报错\n",
    "RuntimeError: Your system has an unsupported version of sqlite3. Chroma requires sqlite3 >= 3.35.0.\n",
    "Please visit https://docs.trychroma.com/troubleshooting#sqlite to learn how to upgrade.\n",
    "请使用python3.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-04T22:05:18.496960Z",
     "start_time": "2023-08-04T22:05:18.469533Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OPENAI_API_KEY=\"sk-y09xxxxxxxxxxx7fU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-04T22:24:58.528889Z",
     "start_time": "2023-08-04T22:24:58.514891Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\pycharmproject\\\\LangChainStudyProject\\\\venv\\\\Scripts'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.path.dirname(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T18:22:20.387941Z",
     "start_time": "2023-08-05T18:22:20.341934Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#设置代理\n",
    "import os\n",
    "\n",
    "os.environ['http_proxy'] = 'http://127.0.0.1:10809'\n",
    "os.environ['https_proxy'] = 'http://127.0.0.1:10809'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T18:22:43.382092Z",
     "start_time": "2023-08-05T18:22:23.154397Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "index = VectorstoreIndexCreator().from_loaders([loader])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T18:22:59.934101Z",
     "start_time": "2023-08-05T18:22:56.940715Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. It can be done using LLM with simple prompting, task-specific instructions, or human inputs. Tree of Thoughts (Yao et al. 2023) is an example of a task decomposition technique that explores multiple reasoning possibilities at each step and generates a tree structure.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.query(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T18:24:32.784638Z",
     "start_time": "2023-08-05T18:24:31.040019Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T18:25:26.425719Z",
     "start_time": "2023-08-05T18:25:26.401183Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 0)\n",
    "all_splits = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T18:25:48.313874Z",
     "start_time": "2023-08-05T18:25:48.265193Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T18:25:54.970471Z",
     "start_time": "2023-08-05T18:25:54.923182Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"LLM Powered Autonomous Agents | Lil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\nemojisearch.app\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      LLM Powered Autonomous Agents\\n    \\nJune 23, 2023\\xa0·\\xa031 min\\xa0·\\xa0Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nAgent System Overview\\n\\nComponent One: Planning\\n\\nTask Decomposition\\n\\nSelf-Reflection\\n\\n\\nComponent Two: Memory\\n\\nTypes of Memory\\n\\nMaximum Inner Product Search (MIPS)\\n\\n\\nComponent Three: Tool Use\\n\\nCase Studies\", metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en'})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_splits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T18:27:10.341148Z",
     "start_time": "2023-08-05T18:27:06.307935Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T18:28:09.730911Z",
     "start_time": "2023-08-05T18:28:09.072610Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What are the approaches to Task Decomposition?\"\n",
    "docs = vectorstore.similarity_search(question)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T18:28:19.263485Z",
     "start_time": "2023-08-05T18:28:19.204815Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.', metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T18:30:29.269981Z",
     "start_time": "2023-08-05T18:30:19.107251Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What are the approaches to Task Decomposition?',\n",
       " 'result': 'The approaches to task decomposition include:\\n\\n1. Prompting with LLM: This approach involves using simple prompts to guide the model in decomposing the task into subgoals or steps. For example, the model can be prompted with instructions like \"Steps for XYZ\" or \"What are the subgoals for achieving XYZ?\"\\n\\n2. Task-specific instructions: In this approach, task-specific instructions are provided to the model to guide the task decomposition process. For example, if the task is to write a novel, the model can be instructed to \"Write a story outline\" as a step in the task decomposition.\\n\\n3. Human inputs: Task decomposition can also be done with the help of human inputs. Humans can provide guidance and input to the model in breaking down the task into smaller and simpler steps.\\n\\nIt\\'s important to note that the context does not provide any additional approaches to task decomposition beyond these three.'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "qa_chain = RetrievalQA.from_chain_type(llm,retriever=vectorstore.as_retriever())\n",
    "qa_chain({\"query\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T18:32:43.589458Z",
     "start_time": "2023-08-05T18:32:43.574929Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T18:32:54.181286Z",
     "start_time": "2023-08-05T18:32:54.150576Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "chat = ConversationalRetrievalChain.from_llm(llm, retriever=retriever, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T18:33:07.098766Z",
     "start_time": "2023-08-05T18:32:59.518982Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Some of the main ideas in self-reflection include:\\n1. Iterative improvement: Self-reflection allows autonomous agents to improve by refining past action decisions and correcting mistakes.\\n2. Trial and error: Self-reflection is crucial in real-world tasks where trial and error are inevitable.\\n3. Two-shot examples: Self-reflection is created by showing pairs of failed trajectories and ideal reflections for guiding future changes in the plan.\\n4. Working memory: Reflections are added to the agent's working memory, up to three, to be used as context for querying.\\n5. Performance evaluation: Self-reflection involves continuously reviewing and analyzing actions, self-criticizing behavior, and reflecting on past decisions and strategies to refine approaches.\\n6. Efficiency: Self-reflection encourages being smart and efficient, aiming to complete tasks in the least number of steps.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = chat({\"question\": \"What are some of the main ideas in self-reflection?\"})\n",
    "result['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T18:33:21.650949Z",
     "start_time": "2023-08-05T18:33:14.731615Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The Reflexion paper handles self-reflection by showing two-shot examples to the Learning Language Model (LLM). Each example consists of a failed trajectory and an ideal reflection that guides future changes in the agent's plan. These reflections are then added to the agent's working memory, up to a maximum of three, to be used as context for querying the LLM. This allows the agent to iteratively improve its reasoning skills by refining past action decisions and correcting previous mistakes.\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = chat({\"question\": \"How does the Reflexion paper handle it?\"})\n",
    "result['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
